---
title: Textbook
---

I am currently working on a textbook aiming to provide a technical introduction to the research areas of Active Inference and the [Free Energy Principle](https://en.wikipedia.org/wiki/Free_energy_principle) (see background section below). The mathematical ideas within this field have the potential to revolutionize many industries and greatly advance theoretical AI research beyond what has already been achieved through deep learning in the last decade. As of July 2023, 10 chapters (of 22 projected chapters) have been written for the first draft along with accompanying (draft) video lectures and Jupyter notebooks. The textbook and its related content is presented weekly to the Active Inference Institute who is hosting my project which has enabled me to receive feedback from other researchers in the field. My intention for this work is to condense over 20 years of progress and literature in the field to bring a wider degree of recognition and interest from machine learning researchers in academia and industry. 

The textbook is aimed at an upper undergraduate/early graduate level and aims to be fairly self-contained, requiring little knowledge of neuroscience and related literature but focusing heavily on mathematics and Python code. The prerequisites for the book are basic Python programming knowledge, probability theory, multivariate calculus, basic linear algebra, and high-school physics, topics usually covered in lower undergraduate science and engineering degrees.

At the moment, the textbook draft is only available to members of the Active Inference Institute "Fundamentals of Active Inference" project. However, as soon as the first draft is written it will be released on this website for further feedback.

## Background

Active Inference is a fast-growing field within computational neuroscience that integrates ideas from machine learning, physics, and other fields to mathematically describe the brain and human/animal behavior. The original ideas were conceived by the neuroscientist [Dr. Karl Friston](https://www.fil.ion.ucl.ac.uk/~karl/) who has worked with a number of collaborators in the last two decades to expand the scope and applicability of the theory to the brain and living systems more generally. Active Inference is based in machine learning and has been successfully applied to many different areas of neuroscience and represents the culmination of decades of prior research. It provides an interdisciplinary way to look at a broad range of fields ranging from robotics, economics, cybernetics/control theory, machine learning and more.

The Free Energy Principle is a broader and more theoretical physics-based theory also created by Friston that attempts to answer the famous question posed by Erwin Schrodinger in 1944: ["What is life?"](https://en.wikipedia.org/wiki/What_Is_Life%3F). The Free Energy Principle provides the theoretical and philosophical scaffold behind Active Inference and a mathematical modeling framework known as [Bayesian Mechanics](https://royalsocietypublishing.org/doi/full/10.1098/rsfs.2022.0029) to model living systems that undergo “self-organization” that is, they stay organized as living being despite changes happening around them that threaten their existence. Under the Free Energy Principle, it is suggested that biological organisms follows the imperative of minimizing a statistical quantity known as *variational free energy* which serves as a universal objective function in approximate Bayesian inference. Organisms that successfully minimize this quantity are the ones that are able to (actively) infer the current (and future) states of their environments and thus restrict themselves to preferable states conducive to their survival.

## Textbook preview

If you would like to preview chapters from the textbook you can access them from the table below.

| Part                                      |Chapter                                                                                  | Preview (PDF) |
|-------------------------------------------|---------------------------------------------------------------------------------------- |---------------|
| Part I. Fundamentals                      | Introduction                                                                            |               |
| Part I. Fundamentals                      | 1. The Hypothesis-Testing Brain                                                         |               |
| Part I. Fundamentals                      | 2. Hidden State Estimation                                                              |               |       
| Part I. Fundamentals                      | 3. Combining Learning and Inference                                                     |               |
| Part I. Fundamentals                      | 4. Variational Bayesian Inference                                                       |               | 
| Part I. Fundamentals                      | 5. Predictive Coding                                                                    |               |
| Part II. Active Inference Core (Low Road) | 6. Generalized filtering for perception                                                 |               |
| Part II. Active Inference Core (Low Road) | 7. Active generalized filtering                                                         |               |
| Part II. Active Inference Core (Low Road) | 8. Learning, attention, and hierarchical generalized filtering                          |               |
| Part II. Active Inference Core (Low Road) | 9. Active inference in partially observable Markov decision processes                   |               |
| Part II. Active Inference Core (Low Road) | 10. Learning and hierarchical models for partially observable Markov decision processes |               |
| Part III. Bayesian Mechanics (High Road)  |                                                                                         |               |